{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Descripción de imágenes con un modelo recurrente (_RNN_)"
   ],
   "metadata": {
    "id": "K2s1A9eLRPEj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este notebook detalla un modelo que utiliza capas recurrentes para subtitulado/descripción de imágenes.\n",
    "\n",
    "Este modelos es simalar a [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)\n",
    "\n",
    "Además las implementación está basada está basada en [Implementation of Attention Mechanism for Caption Generation on Transformers using TensorFlow\n",
    "](https://www.tensorflow.org/tutorials/text/image_captioning)\n"
   ],
   "metadata": {
    "id": "Cffg2i257iMS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***DataSet:*** \n",
    "\n",
    "Este notebook utiliza el conjunto de datos [MS-COCO](http://cocodataset.org/#home) para el entrenamiento y testeo del modelo."
   ],
   "metadata": {
    "id": "QASbY_HGo4Lq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Importar librerías"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T18:47:45.905250Z",
     "iopub.status.busy": "2021-06-16T18:47:45.904691Z",
     "iopub.status.idle": "2021-06-16T18:47:47.492796Z",
     "shell.execute_reply": "2021-06-16T18:47:47.492254Z"
    },
    "id": "U8l4RJ0XRPEm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preparar entorno y el conjunto de datos _MS COCO_\n",
    "\n",
    "Previamente, es necesario haber descargado el conjunto de datos _MS COCO_, crear un directorio \"ms-coco\" y organizar los archivos siguiendo la siguiente estructura;\n",
    "\n",
    "---\n",
    "```\n",
    "ms-coco\n",
    "  annotations\n",
    "  images\n",
    "    train2014\n",
    "    val2014\n",
    "```\n",
    "---\n",
    "\n",
    "En el siguiente código, se verifica la existencia del contenido del directorio ms-coco. Y con la variable de entorno ***CUDA_VISIBLE_DEVICES*** se especifican las GPU a utilizar."
   ],
   "metadata": {
    "id": "b6qbGw8MRPE5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# [IMPORTANTE]: Configurar CUDA_VISIBLE_DEVICES\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2,3\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "root_dir = \"/\".join(os.getcwd().split(\"/\")[0:-1])+\"/\"\n",
    "print(\"INFO: El directorio ráiz de proyecto es:\",root_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "coco_dir=\"ms-coco/\"\n",
    "annotation_folder = \"annotations/\"\n",
    "image_folder = \"images/\"\n",
    "\n",
    "if not os.path.exists(root_dir + coco_dir + annotation_folder) or not os.path.exists(root_dir + coco_dir + image_folder):\n",
    "    raise Exception('ERR: Faltan archivos..' )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cargar _dataset_"
   ],
   "metadata": {
    "id": "aANEzb5WwSzg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(root_dir + coco_dir + annotation_folder + f'/captions_train2014.json') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = root_dir +coco_dir + 'images/train2014/' + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:02:55.012597Z",
     "iopub.status.busy": "2021-06-16T19:02:54.976577Z",
     "iopub.status.idle": "2021-06-16T19:02:55.441054Z",
     "shell.execute_reply": "2021-06-16T19:02:55.441445Z"
    },
    "id": "miER7EHMB3Ge"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(root_dir + coco_dir + '/annotations' + f'/captions_val2014.json') as f:\n",
    "    annotations.update(json.load(f))\n",
    "\n",
    "for val in annotations['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = root_dir + coco_dir + 'images/val2014/' + 'COCO_val2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tamaño del _dataset_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "print('INFO: Tamaño de image_paths:',len(image_paths))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:02:55.485394Z",
     "iopub.status.busy": "2021-06-16T19:02:55.480241Z",
     "iopub.status.idle": "2021-06-16T19:02:55.498517Z",
     "shell.execute_reply": "2021-06-16T19:02:55.498856Z"
    },
    "id": "7vvqkqYGMhvm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    all_captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:02:55.509058Z",
     "iopub.status.busy": "2021-06-16T19:02:55.508205Z",
     "iopub.status.idle": "2021-06-16T19:02:55.510539Z",
     "shell.execute_reply": "2021-06-16T19:02:55.510100Z"
    },
    "id": "hrmdtMX8Lnyh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('INFO: Subtítulo de referencia: '+' '.join(all_captions[0].split(' ')[1:-1]))\n",
    "Image.open(img_name_vector[0])"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:02:55.514587Z",
     "iopub.status.busy": "2021-06-16T19:02:55.513672Z",
     "iopub.status.idle": "2021-06-16T19:02:55.635020Z",
     "shell.execute_reply": "2021-06-16T19:02:55.635439Z"
    },
    "id": "RhCND0bCUP11"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Pre-procesado de las imágenes\n",
    "\n",
    "Para la extracción de características se utiliza la red _InceptionV3_ (que está preentrenado en _ImageNet_). \n",
    "\n",
    "Para lo que es necesario:\n",
    "- Cambiar el tamaño de la imagen a 299px por 299px.\n",
    "- Normalizar las imágenes con [preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input)."
   ],
   "metadata": {
    "id": "8cSW4u-ORPFQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:02:55.640450Z",
     "iopub.status.busy": "2021-06-16T19:02:55.639624Z",
     "iopub.status.idle": "2021-06-16T19:02:55.641883Z",
     "shell.execute_reply": "2021-06-16T19:02:55.641464Z"
    },
    "id": "zXR0217aRPFR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inicializar _InceptionV3_ y cargar los pesos de _ImageNet_ previamente entrenados.\n",
    "\n",
    "Ahora creará un modelo tf.keras donde la capa de salida es la última capa convolucional _InceptionV3_. Y la forma de la salida de esta capa es 8x8x2048.\n"
   ],
   "metadata": {
    "id": "MDvIu4sXRPFV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_model = tf.keras.applications._InceptionV3_(include_top=False,\n",
    "                                                    weights='_ImageNet_')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:02:55.647954Z",
     "iopub.status.busy": "2021-06-16T19:02:55.647363Z",
     "iopub.status.idle": "2021-06-16T19:03:00.408818Z",
     "shell.execute_reply": "2021-06-16T19:03:00.408298Z"
    },
    "id": "RD3vW4SsRPFW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encode = sorted(set(img_name_vector))\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "if not os.path.exists(img_name_vector[0]+'.npy'):\n",
    "    for img, path in tqdm(image_dataset):\n",
    "        batch_features = image_features_extract_model(img)\n",
    "        batch_features = tf.reshape(batch_features,\n",
    "                                  (batch_features.shape[0], -1, \n",
    "                                   batch_features.shape[3]))\n",
    "\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            np.save(path_of_feature, bf.numpy())\n",
    "else:\n",
    "    print(\"INFO: Características en:\", root_dir + coco_dir + 'images/[val2014|train2014]/')\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Pre-procesado y tokenizado de los subtítulos\n",
    "\n",
    "Procedimiento:\n",
    "* Se convierten en tokens los subtítulos.\n",
    "* Se limita el tamaño del vocabulario a las 5.000 palabras principales y reemplazara todas las demás palabras con el token \"UNK\" (desconocido).\n",
    "* Se mapean palabras a índices (word-to-index) e índices a palabras (index-to-word).\n",
    "* Se rellenan todas las secuencias para que tengan la misma longitud que la más larga.\n",
    "\n"
   ],
   "metadata": {
    "id": "nyqH3zFwRPFi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Función para encuentrar la longitud máxima de un subtítulo\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:41.689465Z",
     "iopub.status.busy": "2021-06-16T19:03:41.688894Z",
     "iopub.status.idle": "2021-06-16T19:03:41.691000Z",
     "shell.execute_reply": "2021-06-16T19:03:41.690603Z"
    },
    "id": "HZfK8RhQRPFj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Se eligen las 5000 palabras principales del vocabulario\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n",
    "tokenizer.fit_on_texts(all_captions)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:41.765927Z",
     "iopub.status.busy": "2021-06-16T19:03:41.729501Z",
     "iopub.status.idle": "2021-06-16T19:03:42.059377Z",
     "shell.execute_reply": "2021-06-16T19:03:42.059880Z"
    },
    "id": "oJGE34aiRPFo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:42.064079Z",
     "iopub.status.busy": "2021-06-16T19:03:42.063449Z",
     "iopub.status.idle": "2021-06-16T19:03:42.065687Z",
     "shell.execute_reply": "2021-06-16T19:03:42.065248Z"
    },
    "id": "8Q44tNQVRPFt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Se crea el vector tokenizado\n",
    "all_seqs = tokenizer.texts_to_sequences(all_captions)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:42.109636Z",
     "iopub.status.busy": "2021-06-16T19:03:42.104343Z",
     "iopub.status.idle": "2021-06-16T19:03:42.351210Z",
     "shell.execute_reply": "2021-06-16T19:03:42.351653Z"
    },
    "id": "0fpJb5ojRPFv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(all_seqs, padding='post')"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:42.360337Z",
     "iopub.status.busy": "2021-06-16T19:03:42.359572Z",
     "iopub.status.idle": "2021-06-16T19:03:42.467407Z",
     "shell.execute_reply": "2021-06-16T19:03:42.467827Z"
    },
    "id": "AidglIZVRPF4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_length = calc_max_length(all_seqs)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:42.473887Z",
     "iopub.status.busy": "2021-06-16T19:03:42.473224Z",
     "iopub.status.idle": "2021-06-16T19:03:42.475335Z",
     "shell.execute_reply": "2021-06-16T19:03:42.474848Z"
    },
    "id": "gL0wkttkRPGA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Split del _dataset_ y crear tf.data dataset"
   ],
   "metadata": {
    "id": "M3CD75nDpvTI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_dir=root_dir+\"splits/\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_file(split):\n",
    "    return split_dir + f'karpathy_{split}_images.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_split_image_ids_and_paths(split):\n",
    "    split_df = pd.read_csv(split_file(split), sep=' ', header=None)\n",
    "    dir_aux = root_dir + coco_dir +'images/'+ split_df.iloc[:,0]\n",
    "    return split_df.iloc[:,1].to_numpy(), dir_aux.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "    \n",
    "img_name_train_keys = read_split_image_ids_and_paths('train')[1]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    \n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"INFO: Tamaño del train dataset:\", len(img_name_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_name_val_keys = read_split_image_ids_and_paths('valid')[1] \n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    \n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"INFO: Tamaño del val dataset:\", len(img_name_val))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_name_test_keys = read_split_image_ids_and_paths('test')[1]\n",
    "\n",
    "img_name_test = []\n",
    "\n",
    "for img_test in img_name_test_keys:\n",
    "    img_name_test.extend([img_test])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"INFO: Tamaño del test dataset:\", len(img_name_test))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:42.508206Z",
     "iopub.status.busy": "2021-06-16T19:03:42.507505Z",
     "iopub.status.idle": "2021-06-16T19:03:42.510045Z",
     "shell.execute_reply": "2021-06-16T19:03:42.510409Z"
    },
    "id": "XmViPkRFRPGH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Crear tf.data del dataset\n"
   ],
   "metadata": {
    "id": "uEWM9xrYcg45"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 500\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "print(\"INFO: Número de steps:\", num_steps)\n",
    "\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:42.514418Z",
     "iopub.status.busy": "2021-06-16T19:03:42.513849Z",
     "iopub.status.idle": "2021-06-16T19:03:42.515781Z",
     "shell.execute_reply": "2021-06-16T19:03:42.516146Z"
    },
    "id": "Q3TnZ1ToRPGV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Se utiliza map para cargar los archivos numpy en paralelo\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32])\n",
    "                     )\n",
    "                     \n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:42.549098Z",
     "iopub.status.busy": "2021-06-16T19:03:42.543917Z",
     "iopub.status.idle": "2021-06-16T19:03:43.180044Z",
     "shell.execute_reply": "2021-06-16T19:03:43.179448Z"
    },
    "id": "FDF_Nm3tRPGZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Modelo\n",
    "\n",
    "A continuación:\n",
    "\n",
    "* Se extraen las características de la capa convolucional inferior de _InceptionV3_, resultando un vector con forma de (8, 8, 2048), y que se transforma en (64, 2048).\n",
    "* Se pasa ese vector, a través, del codificador CNN (que consta de una sola capa _Fully Connected_).\n",
    "* Y la red recurrente (GRU) predice la siguiente palabra."
   ],
   "metadata": {
    "id": "nrvoDphgRPGd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        \n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.186027Z",
     "iopub.status.busy": "2021-06-16T19:03:43.185429Z",
     "iopub.status.idle": "2021-06-16T19:03:43.187149Z",
     "shell.execute_reply": "2021-06-16T19:03:43.187613Z"
    },
    "id": "ja2LFTMSdeV3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Este codificador pasa las features a través de una capa Fully Connected \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.191790Z",
     "iopub.status.busy": "2021-06-16T19:03:43.191200Z",
     "iopub.status.idle": "2021-06-16T19:03:43.192974Z",
     "shell.execute_reply": "2021-06-16T19:03:43.193303Z"
    },
    "id": "AZ7R1RxHRPGf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # Se define la atención como un modelo separado\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # Después del embedding x == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Se pasa el vector al GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.199912Z",
     "iopub.status.busy": "2021-06-16T19:03:43.199291Z",
     "iopub.status.idle": "2021-06-16T19:03:43.201373Z",
     "shell.execute_reply": "2021-06-16T19:03:43.200951Z"
    },
    "id": "V9UbGQmERPGi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.205518Z",
     "iopub.status.busy": "2021-06-16T19:03:43.204921Z",
     "iopub.status.idle": "2021-06-16T19:03:43.218795Z",
     "shell.execute_reply": "2021-06-16T19:03:43.218336Z"
    },
    "id": "Qs_Sr03wRPGk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.223465Z",
     "iopub.status.busy": "2021-06-16T19:03:43.222890Z",
     "iopub.status.idle": "2021-06-16T19:03:43.225177Z",
     "shell.execute_reply": "2021-06-16T19:03:43.224728Z"
    },
    "id": "-bYN7xA0RPGl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Training "
   ],
   "metadata": {
    "id": "6A3Ni64joyab"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checkpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rm -rf checkpoints/train/*"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.229220Z",
     "iopub.status.busy": "2021-06-16T19:03:43.228663Z",
     "iopub.status.idle": "2021-06-16T19:03:43.239852Z",
     "shell.execute_reply": "2021-06-16T19:03:43.239305Z"
    },
    "id": "PpJAqPMWo0uE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # Restaurando el último checkpoint en checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.243610Z",
     "iopub.status.busy": "2021-06-16T19:03:43.242984Z",
     "iopub.status.idle": "2021-06-16T19:03:43.244749Z",
     "shell.execute_reply": "2021-06-16T19:03:43.245082Z"
    },
    "id": "fUkbqhc_uObw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "PHod7t72RPGn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_plot = []"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.248254Z",
     "iopub.status.busy": "2021-06-16T19:03:43.247685Z",
     "iopub.status.idle": "2021-06-16T19:03:43.249460Z",
     "shell.execute_reply": "2021-06-16T19:03:43.249827Z"
    },
    "id": "Vt4WZ5mhJE-E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "  \n",
    "    # Inicializar el hidden state para cada batch porque\n",
    "    # los subtítulos no están relacionado de una imágen a otra.\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "  \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "  \n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Se pasan los features por el decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "  \n",
    "            loss += loss_function(target[:, i], predictions)\n",
    " \n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "  \n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "  \n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  \n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "  \n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "  \n",
    "    return loss, total_loss"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.255262Z",
     "iopub.status.busy": "2021-06-16T19:03:43.254700Z",
     "iopub.status.idle": "2021-06-16T19:03:43.256338Z",
     "shell.execute_reply": "2021-06-16T19:03:43.256706Z"
    },
    "id": "sqgyz2ANKlpU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.autograph.set_verbosity(0)\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, EPOCHS)):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "            \n",
    "    # Almacenar la época y la loss \n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "    \n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:03:43.261730Z",
     "iopub.status.busy": "2021-06-16T19:03:43.261028Z",
     "iopub.status.idle": "2021-06-16T19:20:10.177285Z",
     "shell.execute_reply": "2021-06-16T19:20:10.177647Z"
    },
    "id": "UlA4VIQpRPGo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:20:10.193032Z",
     "iopub.status.busy": "2021-06-16T19:20:10.192347Z",
     "iopub.status.idle": "2021-06-16T19:20:10.320554Z",
     "shell.execute_reply": "2021-06-16T19:20:10.320118Z"
    },
    "id": "1Wm83G-ZBPcC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Generar descripción \n"
   ],
   "metadata": {
    "id": "xGvOcLQKghXN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:20:10.327705Z",
     "iopub.status.busy": "2021-06-16T19:20:10.326812Z",
     "iopub.status.idle": "2021-06-16T19:20:10.329127Z",
     "shell.execute_reply": "2021-06-16T19:20:10.328728Z"
    },
    "id": "RCWpDtyNRPGs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(np.ceil(len_result/2), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:20:10.334790Z",
     "iopub.status.busy": "2021-06-16T19:20:10.333944Z",
     "iopub.status.idle": "2021-06-16T19:20:10.336155Z",
     "shell.execute_reply": "2021-06-16T19:20:10.335755Z"
    },
    "id": "fD_y7PD6RPGt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejemplos de imágenes con la descripción generada"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_token = tokenizer.word_index['<start>']\n",
    "end_token = tokenizer.word_index['<end>']\n",
    "# Seleccionar una imágen aleatoria del conjunto de validación.\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = [tokenizer.index_word[i]\n",
    "                for i in cap_val[rid] if i not in [0]]\n",
    "result, attention_plot = generate(image)\n",
    "\n",
    "\n",
    "# Eliminar \"<unk>\" \n",
    "for i in result:\n",
    "    if i==\"<unk>\":\n",
    "        result.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "    if i==\"<unk>\":\n",
    "        real_caption.remove(i)\n",
    "        \n",
    "real_caption = ' '.join(real_caption)\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "        \n",
    "print ('Descripción de referencia:', real_caption)\n",
    "print ('Descripción resultante:', ' '.join(word for word in result[:-1]))\n",
    "temp_image = np.array(Image.open(image))\n",
    "plt.imshow(temp_image)\n",
    "plt.axis('off')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_attention(image, result, attention_plot)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T19:20:10.345417Z",
     "iopub.status.busy": "2021-06-16T19:20:10.344615Z",
     "iopub.status.idle": "2021-06-16T19:20:11.744366Z",
     "shell.execute_reply": "2021-06-16T19:20:11.744738Z"
    },
    "id": "7x8RiPHe_4qI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generar descripciones de la imágenes de test y val"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def f_create_json(img_name, split_val ):\n",
    "    date=str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    \n",
    "    list_pred = []\n",
    "    list_true= []\n",
    "    \n",
    "    idx = 0\n",
    "\n",
    "    for image in tqdm(img_name):\n",
    "        dict_pred = {}\n",
    "        dict_true = {}\n",
    "\n",
    "        regex_expression = r'(?P<prefix>COCO_(train|val)2014_)(?P<number>[0-9]+)'\n",
    "        regex_expression = re.compile(regex_expression)\n",
    "        img_id = int(regex_expression.match(Path(image).stem).group('number'))  \n",
    "        caption_list, _  = generate(image)\n",
    "        \n",
    "        \n",
    "        dict_pred['image_id' ] = img_id\n",
    "        dict_pred['caption' ] = ' '.join(word for word in caption_list[:-1]).replace('<unk>','')\n",
    "        \n",
    "        \n",
    "        list_pred.append(dict_pred)\n",
    "        \n",
    "        if (split_val == True):\n",
    "            dict_true['image_id' ] = img_id\n",
    "            dict_true['caption' ] = ' '.join([tokenizer.index_word[i] for i in cap_val[idx] if i not in [0]])\n",
    "            list_true.append(dict_true)\n",
    "            idx+=1\n",
    "\n",
    "    full_file_name = 'output/rnn-tf-'+date\n",
    "    \n",
    "    with open(full_file_name+'-predictions.json', 'w') as f:\n",
    "        json.dump(list_pred, f)\n",
    "    \n",
    "    print('Archivo con las predicciones:', full_file_name+'-predictions.json')\n",
    "    \n",
    "    if (split_val == True):\n",
    "        with open(full_file_name+'-true.json', 'w') as f:\n",
    "            json.dump(list_true, f)\n",
    "            print('Archivo con las referencias:',full_file_name+'-true.json')\n",
    "            \n",
    "    return dict_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Crear json con las descripciones del conjunto de test dataset\n",
    "dict_pred = f_create_json(img_name_test, split_val = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "aux = ['a', 'building', 'sitting', 'next', 'to', 'a', '<unk>','building', '<end>']\n",
    "for i in aux:\n",
    "    if i==\"<unk>\":\n",
    "        aux.remove(i)\n",
    "        print(aux)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_captioning.ipynb",
   "toc_visible": true
  },
  "interpreter": {
   "hash": "5d0ada31caf5be1f4ba4c9073c1d7fa5c6e47378060b4db2d2ae34cf66fde15e"
  },
  "kernelspec": {
   "display_name": "tf-rnn",
   "language": "python",
   "name": "tf-rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}