{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Descripción de imágenes con un modelo _Transformer (Tensorflow)_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este notebook detalla un modelo _Transformer_ para subtitulado/descripción de imágenes.\n",
    "\n",
    "La arquitectura del modelo es similar a la detallada en el paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). \n",
    "\n",
    "Además la implementación se basa en el _Notebook_ [Caption_Transformer.ipynb](https://github.com/tanishqgautam/Image-Captioning/blob/main/Transformer/Caption_Transformer.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***DataSet:*** \n",
    "\n",
    "Este notebook utiliza el conjunto de datos [MS-COCO](http://cocodataset.org/#home) para el entrenamiento y testeo del modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Importar librerías"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {
    "id": "Si32U9x-8wzq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.utils import shuffle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import datetime\n",
    "from pathlib import Path   \n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preparar entorno y el conjunto de datos _MS COCO_\n",
    "\n",
    "Previamente, es necesario haber descargado el conjunto de datos _MS COCO_, crear un directorio \"ms-coco\" y organizar los archivos siguiendo la siguiente estructura;\n",
    "\n",
    "---\n",
    "```\n",
    "ms-coco\n",
    "  annotations\n",
    "  images\n",
    "    train2014\n",
    "    val2014\n",
    "```\n",
    "---\n",
    "\n",
    "En el siguiente código, se verifica la existencia del contenido del directorio ms-coco. Y con la variable de entorno ***CUDA_VISIBLE_DEVICES*** se especifican las GPU a utilizar."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# [IMPORTANTE]: Configurar CUDA_VISIBLE_DEVICES\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "root_dir = \"/\".join(os.getcwd().split(\"/\")[0:-1])+\"/\"\n",
    "print(\"INFO: El directorio ráiz de proyecto es:\",root_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "coco_dir=\"ms-coco/\"\n",
    "annotation_folder = \"annotations/\"\n",
    "image_folder = \"images/\"\n",
    "\n",
    "if not os.path.exists(root_dir + coco_dir + annotation_folder) or not os.path.exists(root_dir + coco_dir + image_folder):\n",
    "    raise Exception('ERR: Faltan archivos..' )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cargar _dataset_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(root_dir + coco_dir + annotation_folder + f'/captions_train2014.json') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    caption = val['caption']\n",
    "    image_path = root_dir +coco_dir + 'images/train2014/' + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(root_dir + coco_dir + '/annotations' + f'/captions_val2014.json') as f:\n",
    "    annotations.update(json.load(f))\n",
    "\n",
    "for val in annotations['annotations']:\n",
    "    caption = val['caption']\n",
    "    image_path = root_dir + coco_dir + 'images/val2014/' + 'COCO_val2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tamaño del _dataset_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "print('INFO: Tamaño de image_paths:',len(image_paths))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    all_captions.extend(caption_list)\n",
    "    all_img_name_vector.extend([image_path] * len(caption_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.DataFrame({'index': list(range(0, len(all_img_name_vector))),\n",
    "                    'filename': all_img_name_vector,\n",
    "                    'caption': all_captions}\n",
    "                   )\n",
    "\n",
    "uni_filenames = np.unique(data.filename.values)\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "npic = 5\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "\n",
    "count = 1\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "for jpgfnm in uni_filenames[10:15]:\n",
    "    filename = jpgfnm\n",
    "    captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n",
    "    image_load = load_img(filename, target_size=target_size)\n",
    "    \n",
    "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "    \n",
    "    ax = fig.add_subplot(npic,2,count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,len(captions))\n",
    "    for i, caption in enumerate(captions):\n",
    "        ax.text(0,i,caption,fontsize=20)\n",
    "    count += 1\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "id": "MKujA2IkfBLL",
    "outputId": "c24ac98d-89d0-4ab0-c585-a6119613616f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Pre-procesado de las imágenes\n",
    "\n",
    "Para la extracción de características se utiliza la red _InceptionV3_ (que está preentrenado en _ImageNet_). \n",
    "\n",
    "Para lo que es necesario:\n",
    "- Cambiar el tamaño de la imagen a 299px por 299px.\n",
    "- Normalizar las imágenes con [preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inicializar _InceptionV3_ y cargar los pesos de _ImageNet_ previamente entrenados.\n",
    "\n",
    "Ahora creará un modelo tf.keras donde la capa de salida es la última capa convolucional _InceptionV3_. Y la forma de la salida de esta capa es 8x8x2048.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_model = tf.keras.applications._InceptionV3_(include_top=False,\n",
    "                                                weights='_ImageNet_')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encode_train = sorted(set(all_img_name_vector))\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "if not os.path.exists(all_img_name_vector[0]+'.npy'):\n",
    "    for img, path in tqdm(image_dataset):\n",
    "        batch_features = image_features_extract_model(img)\n",
    "        batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "        \n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            np.save(path_of_feature, bf.numpy())\n",
    "        \n",
    "else:\n",
    "    print(\"INFO: Características en:\", root_dir + coco_dir + 'images/[val2014|train2014]/')\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Pre-procesado de los subtítulos\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocabulary = []\n",
    "for txt in data.caption.values:\n",
    "    vocabulary.extend(txt.split())\n",
    "print('INFO: Tamaño del vocabulario: %d' % len(set(vocabulary)))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZfHfLEIgRra",
    "outputId": "e77c4fb5-a7dd-4e2e-cbe3-ac36d83e6a47"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_captions = []\n",
    "\n",
    "for caption  in data[\"caption\"].astype(str):\n",
    "    caption = '<start> ' + caption+ ' <end>'\n",
    "    all_captions.append(caption)\n",
    "all_captions[:10]"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUaf2Bc4BLBh",
    "outputId": "2105ee37-a134-4aba-e534-0e44a1541d2f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_img_name_vector = []\n",
    "\n",
    "for annot in data[\"filename\"]:\n",
    "    full_image_path = annot\n",
    "    all_img_name_vector.append(full_image_path)\n",
    "all_img_name_vector[:10]"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kABgi0fFvVj",
    "outputId": "eb4637e6-3c83-4661-af83-d0334758e91c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"INFO: Tamaño de all_img_name_vector = {len(all_img_name_vector)}\")\n",
    "print(f\"INFO: Tamaño de all_captions = {len(all_captions)}\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SI-ZAolSGJv-",
    "outputId": "f540db0e-9c0f-4409-adab-488efc6af90d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [OPCIONAL]: Limitar el conjunto de datos \n",
    "La función _\"data_limiter\"_ permite limitar el conjunto de datos, para reducir el tiempo del entrenamiento."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def data_limiter(num,\n",
    "                 total_captions,\n",
    "                 all_img_name_vector):\n",
    "    \n",
    "    train_captions, img_name_vector = shuffle(\n",
    "        total_captions,all_img_name_vector,random_state=1)\n",
    "    \n",
    "    train_captions = train_captions[:num]\n",
    "    \n",
    "    img_name_vector = img_name_vector[:num]\n",
    "    \n",
    "    return train_captions,img_name_vector"
   ],
   "outputs": [],
   "metadata": {
    "id": "bxxePX6TGTuF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# [OPCIONAL]\n",
    "# captions, img_name_vector = data_limiter(40000, \n",
    "#                                          all_captions, \n",
    "#                                          all_img_name_vector)\n",
    "\n",
    "# Si no se quiere limitar el conjunto de datos se igualan las \n",
    "# variables para facilitar el cambio.\n",
    "captions = all_captions\n",
    "img_name_vector = all_img_name_vector"
   ],
   "outputs": [],
   "metadata": {
    "id": "qgNpDXZTHXT2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"INFO: Tamaño de img_name_vectorimg_name_vector = {len(img_name_vector)}\")\n",
    "print(f\"INFO: Tamaño de captions = {len(captions)}\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW4pJ3TVuYQj",
    "outputId": "5222a14b-a12d-4d96-9b0c-e329087a83ad"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-procesado y tokenizado de los subtítulos\n",
    "\n",
    "Procedimiento:\n",
    "* Se convierten en tokens los subtítulos.\n",
    "* Se limita el tamaño del vocabulario a las 5.000 palabras principales y reemplazara todas las demás palabras con el token \"UNK\" (desconocido).\n",
    "* Se mapean palabras a índices (word-to-index) e índices a palabras (index-to-word)."
   ],
   "metadata": {
    "id": "sMLfDYAfdH93"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Se eligen las 5000 palabras principales del vocabulario\n",
    "top_k = 5000\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(captions)"
   ],
   "outputs": [],
   "metadata": {
    "id": "iWXKqpL5dBss"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ],
   "outputs": [],
   "metadata": {
    "id": "xq14GqxndRn6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Se crea el vector tokenizado\n",
    "all_seqs = tokenizer.texts_to_sequences(captions)"
   ],
   "outputs": [],
   "metadata": {
    "id": "B85IjQRQdWCV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(all_seqs, padding='post')"
   ],
   "outputs": [],
   "metadata": {
    "id": "-qGqYT73dmmb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Split del _dataset_ y crear tf.data del dataset"
   ],
   "metadata": {
    "id": "zVq2KakYdVN9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_dir=root_dir+\"splits/\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_file(split):\n",
    "    return split_dir + f'karpathy_{split}_images.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_split_image_ids_and_paths(split):\n",
    "    split_df = pd.read_csv(split_file(split), sep=' ', header=None)\n",
    "    dir_aux = root_dir + coco_dir +'images/'+ split_df.iloc[:,0]\n",
    "    return split_df.iloc[:,1].to_numpy(), dir_aux.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "    \n",
    "img_name_train_keys = read_split_image_ids_and_paths('train')[1]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    \n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"INFO: Tamaño del train dataset:\", len(img_name_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_name_val_keys = read_split_image_ids_and_paths('valid')[1] \n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "\n",
    "\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    \n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"INFO: Tamaño del val dataset:\", len(img_name_val))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_name_test_keys = read_split_image_ids_and_paths('test')[1]\n",
    "\n",
    "img_name_test = []\n",
    "\n",
    "for img_test in img_name_test_keys:\n",
    "    img_name_test.extend([img_test])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"INFO: Tamaño del test dataset:\", len(img_name_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a tf.data dataset for training"
   ],
   "metadata": {
    "id": "iUGdUhcfeAq8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "print(\"INFO: Número de steps:\", num_steps)"
   ],
   "outputs": [],
   "metadata": {
    "id": "qU_j7m48eGSn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap"
   ],
   "outputs": [],
   "metadata": {
    "id": "anmVgUjzeZOw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Se utiliza map para cargar los archivos numpy en paralelo\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {
    "id": "YlChoPCGeab0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Modelo\n",
    "\n",
    "### _Positional Encoding_\n",
    "\n",
    "Se inserta información de la posición (_positional encoding_) relativa o absoluta de los tokens de la secuencia para mantener el orden de dicha secuencia. \n",
    "En esta caso, se utiliza para el _positional encoding_ funciones seno y coseno de diferentes frecuencias."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    \n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ],
   "outputs": [],
   "metadata": {
    "id": "xRFegUqmINJr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def positional_encoding_1d(position, d_model):\n",
    "    \n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # Función seno para índices pares en el array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Función coseno para índices impares en el array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "      \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "      \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ],
   "outputs": [],
   "metadata": {
    "id": "zNARmUUYMbyi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def positional_encoding_2d(row, col, d_model):\n",
    "    assert d_model % 2 == 0\n",
    "    \n",
    "    row_pos = np.repeat(np.arange(row),col)[:,np.newaxis]\n",
    "    col_pos = np.repeat(np.expand_dims(np.arange(col),0),row,axis=0).reshape(-1,1)\n",
    "    angle_rads_row = get_angles(row_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
    "    angle_rads_col = get_angles(col_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
    "    \n",
    "    angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n",
    "    angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n",
    "    angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n",
    "    angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n",
    "    pos_encoding = np.concatenate([angle_rads_row,angle_rads_col],axis=1)[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "WAjxo7YlMg6p"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _Multi-Head Attention_\n",
    "\n",
    "En la capa de _“Multi-Head Attention”_ se realiza una proyección lineal de las consultas (Q), claves (K) y valores (V) de h veces. En las que cada vez se utilizan proyecciones lineales diferentes, adaptadas a las dimensiones de dq, dk y dv. \n",
    "\n",
    "Para cada una de estas versiones proyectadas se aplica en paralelo la función de atención _“Scaled Dot-Product Attention”_. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    # Agregar dimensiones adicionales para ampliar el padding de la atención\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ],
   "outputs": [],
   "metadata": {
    "id": "TPtqOKOfiNM6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Od9Ltf9qkJiu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth)\n",
    "      k: key shape == (..., seq_len_k, depth)\n",
    "      v: value shape == (..., seq_len_v, depth_v)\n",
    "      mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "      \n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  #adding -Inf where mask is 1 s.t. value get ignored in softmax\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ],
   "outputs": [],
   "metadata": {
    "id": "0jPObHU0k2SC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "          \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \n",
    "        \"\"\"\n",
    "        Dividir la última dimensión en (num_heads, depth) y\n",
    "        transponer el resultado de la siguiente manera: (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "      \n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "            \n",
    "        return output, attention_weights"
   ],
   "outputs": [],
   "metadata": {
    "id": "Iduottdan-sp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ],
   "outputs": [],
   "metadata": {
    "id": "aF7Zq49KqvDw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Capa codificador-decodificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()     \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)     \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "      \n",
    "    def call(self, x, training, mask=None):     \n",
    "        attn_output, _ = self.mha(x, x, x, mask)  \n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  \n",
    "        \n",
    "        ffn_output = self.ffn(out1)  \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  \n",
    "        \n",
    "        return out2"
   ],
   "outputs": [],
   "metadata": {
    "id": "ks9Oo02jsN3D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "               look_ahead_mask=None, padding_mask=None):\n",
    "\n",
    "        # Usar ahead mask para que durante self attention se considere el token futuro.\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  \n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # Usar padding mask para evitar valores padding de enc_output y dec_input\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  \n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        \n",
    "        out2 = self.layernorm2(attn2 + out1)  \n",
    "        \n",
    "        ffn_output = self.ffn(out2)  \n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        \n",
    "        out3 = self.layernorm3(ffn_output + out2)  \n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ],
   "outputs": [],
   "metadata": {
    "id": "Z3ZXI1RVw2Rf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                   row_size,col_size,rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Dense(self.d_model,activation='relu')\n",
    "        self.pos_encoding = positional_encoding_2d(row_size,col_size, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Añadir embedding y position encoding.\n",
    "        x = self.embedding(x)  \n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  "
   ],
   "outputs": [],
   "metadata": {
    "id": "M6wYw3lpyYWL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                   maximum_position_encoding, rate=0.1):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "               look_ahead_mask=None, padding_mask=None):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        return x, attention_weights"
   ],
   "outputs": [],
   "metadata": {
    "id": "uOQ_DNET2kgO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _Transformer_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,row_size,col_size, \n",
    "                   target_vocab_size,max_pos_encoding, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,row_size,col_size, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size,max_pos_encoding, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training,look_ahead_mask=None, dec_padding_mask=None,enc_padding_mask=None):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  \n",
    "\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  \n",
    "\n",
    "        return final_output, attention_weights"
   ],
   "outputs": [],
   "metadata": {
    "id": "Tv5-Vrse4_uK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hiperparámetros del modelo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_layer = 4\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "row_size = 8\n",
    "col_size = 8\n",
    "target_vocab_size = top_k + 1\n",
    "dropout_rate = 0.1"
   ],
   "outputs": [],
   "metadata": {
    "id": "Qz2eNi5G7RkQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ],
   "outputs": [],
   "metadata": {
    "id": "TsjF1eoTT3eQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ],
   "outputs": [],
   "metadata": {
    "id": "8SSH-4bOTJff"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ],
   "outputs": [],
   "metadata": {
    "id": "0JHB2oUSUmX5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ],
   "outputs": [],
   "metadata": {
    "id": "AkkzdkRvUh4A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ],
   "outputs": [],
   "metadata": {
    "id": "aqXuReTiU-fq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transformer = Transformer(num_layer,d_model,num_heads,dff,row_size,col_size,target_vocab_size,max_pos_encoding=target_vocab_size,rate=dropout_rate)"
   ],
   "outputs": [],
   "metadata": {
    "id": "xM9u-U3BVEW0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_masks_decoder(tar):\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    return combined_mask"
   ],
   "outputs": [],
   "metadata": {
    "id": "D3LHMBwxVohZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_plot = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_step(img_tensor, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    dec_mask = create_masks_decoder(tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(img_tensor, tar_inp, \n",
    "                                   True,  \n",
    "                                   dec_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ],
   "outputs": [],
   "metadata": {
    "id": "CW12LgraY3sP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for epoch in tqdm(range(20)):\n",
    "    \n",
    "    start = time.time()\n",
    "  \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (img_tensor, tar)) in enumerate(dataset):\n",
    "        train_step(img_tensor, tar)\n",
    "    \n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "   \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uC41CqxIaadN",
    "outputId": "b386e75c-7871-4e41-aa3f-05c003450316"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "date=str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transformer.save_weights('model/image_caption_transformer'+date+'.h5')"
   ],
   "outputs": [],
   "metadata": {
    "id": "-pbj0bdG1JTv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Generar descripción "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate(image):\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    start_token = tokenizer.word_index['<start>']\n",
    "    end_token = tokenizer.word_index['<end>']\n",
    "\n",
    "    # Se selecciona como entrada del decodificador el start_token\n",
    "    decoder_input = [start_token]\n",
    "    output = tf.expand_dims(decoder_input, 0) #tokens\n",
    "    result = [] # lista de palabras\n",
    "\n",
    "    for i in range(100):\n",
    "        dec_mask = create_masks_decoder(output)\n",
    "\n",
    "        predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]  \n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == end_token:\n",
    "            return result,tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        result.append(tokenizer.index_word[int(predicted_id)])\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return result,tf.squeeze(output, axis=0), attention_weights"
   ],
   "outputs": [],
   "metadata": {
    "id": "O5r6nWrh1D5r"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejemplos de imágenes con la descripción generada"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_token = tokenizer.word_index['<start>']\n",
    "end_token = tokenizer.word_index['<end>']\n",
    "# Seleccionar una imágen aleatoria del conjunto de validación.\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = [tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]]\n",
    "caption,result,attention_weights = generate(image)\n",
    "\n",
    "# Eliminar \"<unk>\" \n",
    "for i in caption:\n",
    "    if i==\"<unk>\":\n",
    "        caption.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "    if i==\"<unk>\":\n",
    "        real_caption.remove(i)\n",
    "                            \n",
    "real_caption = ' '.join(real_caption)\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "        \n",
    "print ('Descripción de referencia:', real_caption)\n",
    "print ('Descripción resultante:', ' '.join(word for word in caption[:-1]))\n",
    "temp_image = np.array(Image.open(image))\n",
    "plt.imshow(temp_image)\n",
    "plt.axis('off')\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "XkQuofIR3XXy",
    "outputId": "b53af3b2-c21d-4190-e6cf-3dfb69c7b724"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_token = tokenizer.word_index['<start>']\n",
    "end_token = tokenizer.word_index['<end>']\n",
    "# select random image from validation data\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = [tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]]\n",
    "caption,result,attention_weights = generate(image)\n",
    "\n",
    "# Eliminar \"<unk>\"\n",
    "for i in caption:\n",
    "    if i==\"<unk>\":\n",
    "        caption.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "    if i==\"<unk>\":\n",
    "        real_caption.remove(i)\n",
    "\n",
    "real_caption = ' '.join(real_caption)\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "        \n",
    "print ('Descripción de referencia:', real_caption)\n",
    "print ('Descripción resultante:', ' '.join(word for word in caption[:-1]))\n",
    "temp_image = np.array(Image.open(image))\n",
    "plt.imshow(temp_image)\n",
    "plt.axis('off')\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "dvUa6VCHSEzd",
    "outputId": "b35943ae-374d-4e43-ff82-27055b70bdba"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_token = tokenizer.word_index['<start>']\n",
    "end_token = tokenizer.word_index['<end>']\n",
    "# select random image from validation data\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = [tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]]\n",
    "caption,result,attention_weights = generate(image)\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in caption:\n",
    "    if i==\"<unk>\":\n",
    "        caption.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "    if i==\"<unk>\":\n",
    "        real_caption.remove(i)\n",
    "\n",
    "real_caption = ' '.join(real_caption)\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "        \n",
    "print ('Descripción de referencia:', real_caption)\n",
    "print ('Descripción resultante:', ' '.join(word for word in caption[:-1]))\n",
    "temp_image = np.array(Image.open(image))\n",
    "plt.imshow(temp_image)\n",
    "plt.axis('off')\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "NG64jntxSUIf",
    "outputId": "d693cdec-82c4-4810-a570-4cc3632a12c2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_token = tokenizer.word_index['<start>']\n",
    "end_token = tokenizer.word_index['<end>']\n",
    "# Seleccionar una imagen aleatoria del conjunto de validación\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = [tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]]\n",
    "caption,result,attention_weights = generate(image)\n",
    "\n",
    "# Eliminar \"<unk>\"\n",
    "for i in caption:\n",
    "    if i==\"<unk>\":\n",
    "        caption.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "    if i==\"<unk>\":\n",
    "        real_caption.remove(i)\n",
    "\n",
    "real_caption = ' '.join(real_caption)\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "        \n",
    "print ('Descripción de referencia:', real_caption)\n",
    "print ('Descripción resultante:', ' '.join(word for word in caption[:-1]))\n",
    "temp_image = np.array(Image.open(image))\n",
    "plt.imshow(temp_image)\n",
    "plt.axis('off')\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "KcAWvXQ2SUQI",
    "outputId": "f2da86af-0051-4f0d-f243-a1f5ef094277"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def f_create_json(img_name, split_val ):\n",
    "    \n",
    "    \n",
    "    list_pred = []\n",
    "    list_true= []\n",
    "    \n",
    "    idx = 0\n",
    "\n",
    "    for image in tqdm(img_name):\n",
    "        dict_pred = {}\n",
    "        dict_true = {}\n",
    "\n",
    "        regex_expression = r'(?P<prefix>COCO_(train|val)2014_)(?P<number>[0-9]+)'\n",
    "        regex_expression = re.compile(regex_expression)\n",
    "        img_id = int(regex_expression.match(Path(image).stem).group('number'))  \n",
    "        caption_list, _, _  = generate(image)\n",
    "        \n",
    "        for i in caption_list:\n",
    "            if i==\"<unk>\":\n",
    "                caption_list.remove(i)\n",
    "                \n",
    "        dict_pred['image_id' ] = img_id\n",
    "        dict_pred['caption' ] = ' '.join(word for word in caption_list[:-1])\n",
    "        \n",
    "        if (split_val == True):\n",
    "            dict_true['image_id' ] = img_id\n",
    "            dict_true['caption' ] = ' '.join([tokenizer.index_word[i] for i in cap_val[idx] if i not in [0]])\n",
    "            list_true.append(dict_true)\n",
    "\n",
    "        list_pred.append(dict_pred)\n",
    "      \n",
    "\n",
    "        idx+=1\n",
    "        \n",
    "    full_file_name = 'output/transformer-tf-'+date\n",
    "    \n",
    "    with open(full_file_name+'-predictions.json', 'w') as f:\n",
    "        json.dump(list_pred, f)\n",
    "    \n",
    "    print('Archivo con las predicciones:', full_file_name+'-predictions.json')\n",
    "    \n",
    "    if (split_val == True):\n",
    "        with open(full_file_name+'-true.json', 'w') as f:\n",
    "            json.dump(list_true, f)\n",
    "            print('Archivo con las referencias:',full_file_name+'-true.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Crear json con las descripciones del conjunto de test dataset\n",
    "f_create_json(img_name_test, split_val = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Caption_Transformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf-Transformer",
   "language": "python",
   "name": "tf-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}